# 데이터 모델링
매핑은 색인될 문서의 데이터 모델링이라고 할 수 있다
## 매핑 API
매핑은 색인 시 데이터가 어디에 어떻게 저장될지를 결정하는 설정이다. DB의 스키마에 대응하는 개념으로 인덱스에 추가되는 각 데이터 타입을 구체적으로 정의하는 일이다.    
엘라스틱서치는 기본적으로 스키마리스이기 떄문에 명시적으로 필드를 정의하지 않아도 데이터 유형에 따라 필드 데이터 타입에 대한 매핑 정보가 자동으로 생성된다.     
근데 자동 매핑 방식이 잘못될 경우, 생성된 매핑의 타입을 변경이 안되기 때문에 처음 부터 다시 정의 해야 한다    
### 매핑 파라미터
색인할 필드의 데이터를 어떻게 저장할지에 대한 다양한 옵션 제공
- analyzer
해당 필드 데이터를 형태소 분석 
- normalizer
term query에 분석기를 사용하기 위해 사용. cafe, Cafe, caFe 를 같은 데이터로 인식되게 한다
- boost
필드에 가중치를 부여한다. 최신 엘라스틱서치는 색인 시 boost 설정을 할 수 없도록 바뀌었다
- coerce
색인 시 자동 변환을 허용할지 여부를 설정하는 파라미터다. 자동 형변환
- copy_to
매핑 파라미터를 추가한 필드의 값을 지정한 필드로 복사. 여러 개의 필드 데이터를 하나의 필드로 모은다
- fielddata
엘라스틱서치가 힙 공간에 생성하는 메모리 캐시다. 
- doc_values
text 타입을제외한 모든 타입에서 기본적으로 doc_values 캐시를 사용한다
- dynamic
매핑에 필드를 추가할 때 동적으로 생성할지, 생성하지 않을지를 결정
- enabled
메타 성격의 데이터라서 색인하고 싶지 않을 때, 검색은 되지만 색인은 하지 않을 때
- format
날짜 / 시간 포맷
- ignore_above
- ignore_malformed
- index
- fields
- norms
- null_value
- position_increment_gap
- properties
- search_analyzer
- similiarity
    - BM25
    - classic ( TF/IDF ) 
    - boolean
- store
- term_vector
## 메타 필드
엘라스틱서치에서 생성한 문서에서 제공하는 특별한 필드다. 메타 데이터를 저장하는 특수 목적의 필드로서 이를 사용하면 검색 시 문서를 다양한 형태로 제어하는 것이 가능해진다.   
### _index 메타 필드
해당 문서가 속한 인덱스의 이름 ( 없어 진다는 데 ? )
### _type 메타 필드
해당 문서가 속한 매핑 타입 정보 ( 없어 진다는 데 ? )
### _id 메타 필드
문서를 식별하는 유일한 키 값이다 ( 없어 진다는 데 ? )
### _uid 메타 필드
특수한 목적의 식별키 
### _source 메타 필드
문서의 원본 데이터를 제공한다
### _all 메타 필드
색인에 사용된 모든 필드의 정보를 가진 메타 필드
### _routing 메타 필드
특정 문서를 특정 샤드에 저장하기 위해 사용자가 지정하는 메타
## 필드 데이터 타입
엘라스틱서치에서 제공하는 데이터 타입으로 어떠한 종류가 있는지 정확하게 이해하기
### Keyword
키워드 형태로 사용할 데이터에 적합한 데이터 타입    
- 검색 시 필터링 되는 항목
- 정렬이 필요한 항목
- 집계 해야 하는 항목
### Text
분석기가 칼럼의 데이터를 문자열 데이터로 인식하고 이를 분석한다   
### Array
### Numeric
### Date
### Range
### Boolean
### Geo-Point
### IP
### Object
### Nested

## 엘라스틱서치 분석기
특정 단어를 검색했을 때 결과가 없거나 예기치 않은 결과가 나오는 경우가 생기는데, 실제 인덱스의 정보가 어떻게 저장돼 있는지 이해하지 못하고 분석기를 구성했을 확률이 높다  
### 개요
루씬을 기반으로 구축된 텍스트 기반 검색엔진. 루씬이 제공하는 분석기를 그대로 활용한다    
엘라스틱서치는 텍스트를 처리하기 위해 기본적으로 분석기를 사용하기 떄문에 생각하는 대로 동작하지 않는다    
문서를 색인하기 전에 해당 문서의 필드 타입이 무엇인지 확인하고 텍스트 타입이면 분석기를 이용해 이를 분석한다    
텍스트가 분석되면 개별 텀으로 나뉘어 형태소 형태로 분석된다      
엘라스틱서치에서 제공하는 **Analye API**를 이용할 수 있다      
Standard Analyzer는 별도의 형태소 분석은 이뤄지지 않았다     
### 역색인 구조
특정 단어가 등장하는 페이지를 쉽게 찾아갈 수 있다. 루씬의 색인을 물려받아 특수한 방식으로 구조화돼 있다
- 모든 문서가 가지는 단어의 고유 단어 목록
- 해당 단어가 어떤 문서에 속해 있는지에 대한 정보
- 전체 문서에 각 단어가 몇 개 들어있는지에 대한 정보
- 하나의 문서에 단어가 몇 번씩 출현했는지에 대한 빈도

> **문서1**    
elasticsearch is cool    
**문서2**    
Elasticsearch is great


1. 먼저 각 문서를 토근화해야 한다.
2. 토큰화된 단어에 대해 문서 상의 위치와 출현 빈도 등의 정보를 체크해서 표를 만든다.
---
| 토큰 | 문서번호 | 텀의 위치 (Position) | 텀의 빈도 (Term Frequency) |
| ----- | ----- | ----- | ----- |
| elasticsearch | 문서1 | 1 | 1 |
| Elasticsearch | 문서2 | 1 | 1 |
| is | 문서1, 문서2 | 2,2 | 2 |
| cool | 문서1 | 3 | 1 |
| great | 문서2 | 3 | 1 |
---

3. 표를 참고해 검색어가 존재하는 문서를 찾기 위해 검색어와 동일한 토큰을 찾아 해당 토큰의 문서 번호를 찾아가면 된다

> 근데 Elasticsearch 와 elasticsearch를 서로 다른 거로 인식한다

가장 간단한 방법은 색인 전에 텍스트 전체를 소문자로 변환한 다음 색인하는 것이다 
### 분석기의 구조
1. 문장을 특정한 규칙에 의해 수정한다.
2. 수정한 문장을 개별 토큰으로 분리한다.
3. 개별 토큰을 특정한 규칙에 의해 변경한다.
   
    
       
CHARACTER FILTER    
문장 분석 전에 입력 텍스트에 의해 특정한 단어를 변경하거나 HTML과 같은 태그를 제거하는 역할을 하는 필터. 토큰화 전의 전처리 과정    
    
TOKENIZER FILTER    
텍스트를 어떻게 나눌 것인지를 정의한다. 한글을 분해할 떄는 한글 형태소 분석기의 Tokenizer를 사용하고, 상황에 맞게 적절한 Tokenizer를 사용    
    
TOKEN FILTER      
토큰화된 단어를 하나씩 필터링해서 사용자가 원하는 토큰으로 변환한다     
불필요한 단어를 제거하거나 동의어 사전, 변환
   

**전체 분석 프로세스**      
문장 -> Character Filter -> 가공된 문장 -> Tokenizer Filter -> Terms -> Token Filter -> 변경된 Terms -> Index     
Token Filter <-> 사전

분석기는 데이터의 특성에 따라 원하는 분석 결과를 미리 예상해보고 해당 결과를 얻기 위한 옵션을 적용해 설정해야 한다. 경험으로 커버해야 하는 문제란다   

**색인과 검색 시 분석기를 각각 설정**    
분석기를 색인할 때 사용되는 Index Analyzer와 검색할 때 사용되는 Search Analyzer로 구분해서 다시 구성할 수도 있다.

### 대표적인 분석기
- Standard Analyzer
- Whitespace 분석기 - 공백을 기준으로
- Keyword 분석기 - 전체 문자열을 하나의 키워드로
### 전처리 필터
사실 토크나이저 내부에서도 전처리가 가능하기 떄문에 엘라스틱서치에서 공식적으로 제공하는 전처리 필터의 종류도 그리 많지 않다.
- Html strip char 필터
### 토크나이저 필터
분석기를 구성하는 가장 핵심 구성요소다. 분석기에서 어떠한 토크나이저를 사용하느냐에 따라 분석기의 전체적인 성격이 결정된다.    
- Standard 토크나이저
- WHITESPACE TOKENIZER
- Ngram 토크나이저
- Edge Ngram 토크나이저
- Keyword 토크나이저

### 토큰 필터
토크나이저에서 분리된 토큰들을 변형하거나 추가, 삭제할 때 사용하는 필터다.    
토큰이 모두 분리돼야 비로소 동작하기 떄문에 독립적으로는 사용할 수 없다
- Ascii Folding 토큰 필터
- Lowercase 토큰 필터
- Uppercase 토큰 필터
- Stop 토큰 필터
- Stemmer 토큰 필터
- Synonym 토큰 필터
- Trim 토큰 필터
### 동의어 사전

검색 엔진에서 다루는 분야가 많아지면 많아질수록 동의어의 수도 늘어난다     
엘라스틱서치에서 가장 까다로운 부분 중 하나가 바로 동의어를 관리하는 거란다     
동의어를 추가하는 방식에는
1. 동의어를 매핑 설정 정보에 미리 파라미터로 등록하는 방식
2. 특정 파일을 별도로 생성해서 관리하는 방식
첫 번째 방식은 실무에서는 잘 사용되지 않는 단다
- 동의어 사전 만들기
  - 동의어 추가
  - 동의어 치환 
## Document API
엘라스틱서치에서 제공하는 대표적인 Document API
- Index API : 문서를 생성
- Get API : 문서를 조회
- Delete API : 문서를 삭제
- Update API : 문서를 수정
- Bulk API : 대량의 문서를 처리
- Reindex API : 문서를 복사
  
### 문서 파라미터
- 문서 ID 자동 생성
- 버전 관리    
Update API를 이용할 경우 내부적으로 스냅숏을 생성해서 문서를 수정하고 인덱스에 다시 재색인하는데, 이때 버전 정보를 이용한다.    
버전은 1부터 시작해서 도큐먼트가 갱신/삭제될 때마다 증가한다 